{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 支持向量机的三层境界"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 了解SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 函数间隔（Functional margin）与几何间隔（Geometrical margin） "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在超平面$w^Tx+b=0$确定的情况下，$\\vert{} w^Tx+b \\vert{}$能够表示点$x$到超平面的距离，而通过观察$w^Tx+b$的符号与类标记$y$的符号是否一致可判断分类是否正确，所以，可以用$y*(w^Tx+b)$的正负性来判定或表示分类的正确性。于此，我们便引出了**函数间隔（functional margin）**的概念。\n",
    "\n",
    "**函数间隔**（用$\\hat{\\gamma}$表示）为：\n",
    "\n",
    "$$\\hat{\\gamma} = y(w^Tx + b) = yf(x)$$\n",
    "\n",
    "而超平面$(w，b)$关于训练数据集$T$中所有样本点$(x_i，y_i)$的函数间隔最小值（其中$x$是特征，$y$是结果，$i$表示第$i$个样本），便为超平面$(w, b)$关于训练数据集T的**函数间隔**：\n",
    "\n",
    "$$\\hat{\\gamma} = min\\hat{\\gamma_i, i = 1,...,n}$$\n",
    "\n",
    "但这样定义的函数间隔有问题，即如果成比例的改变$w$和$b$（如将它们改成$2w$和$2b$），则函数间隔的值$f(x)$却变成了原来的2倍（虽然此时超平面没有改变），所以只有函数间隔还远远不够。\n",
    "\n",
    "事实上，我们可以对法向量$w$加些约束条件，从而引出真正定义点到超平面的距离-**几何间隔（geometrical margin）**的概念。\n",
    "\n",
    "假定对于一个点$x$，令其垂直投影到超平面上的对应点为$x_0$，$w$是垂直于超平面的一个向量，$\\gamma{}$为样本$x$到超平面的距离，如下图所示：\n",
    "\n",
    "![Text](svm_geometric_margin.png)\n",
    "\n",
    "根据平面几何知识，有\n",
    "\n",
    "![Text](svm_dist_function.png)\n",
    "\n",
    "其中$||w||$为$w$的二阶范数（范数是一个类似于模的表示长度的概念），$\\frac{w}{||w||}$是单位向量。\n",
    "\n",
    "又由于$x_0$是超平面上的点，满足$f(x_0) = 0$，代入超平面的方程$w^Tx + b = 0$可得$w^Tx_0 + b = 0$，即$w^Tx_0 = -b$。\n",
    "\n",
    "随即让此式$ x = x_0 + \\gamma{}\\frac{w}{||w||} $的两边同时乘以$w^T$，再根据$w^Tx_0=-b$和$w^Tw=||w||^2$，即可算出$\\gamma{}$：\n",
    "$$\n",
    "\\gamma{} = \\frac{w^Tx + b}{||w||} = \\frac{f(x)}{||w||}\n",
    "$$\n",
    "\n",
    "为了得到距离$\\gamma{}$的真实值，令$\\gamma{}$乘上对应的类别$y$，即可得出**几何间隔**（用$\\hat{\\gamma}$表示）的定义：\n",
    "\n",
    "**几何间隔：**\n",
    "$$\n",
    "\\tilde{\\gamma} = y\\gamma{} = \\frac{\\hat{\\gamma}}{||w||}\n",
    "$$\n",
    "\n",
    "从上述函数间隔和几何间隔的定义可以看出：几何间隔就是函数间隔除以$||w||$，而且函数间隔$y*(w^Tx+b) = y*f(x)$只是人为定义的间隔度量，几何间隔$\\frac{|f(x)|}{||w||}$是直观上点到超平面的距离。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最大间隔分类器Maximum Margin Classifier的定义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对一个数据点进行分类，当超平面离数据点的“间隔”越大，分类的确信度（confidence）也越大。所以，为了使得分类的确信度尽量高，需要让所选择的超平面能够最大化这个“间隔”值。这个间隔就是下图中的Gap的一半。\n",
    "\n",
    "![Text](svm_biggest_gap.png)\n",
    "\n",
    "通过由前面的分析可知：函数间隔不适合用来最大化间隔值，因为在超平面固定以后，可以等比例地缩放$w$的长度和$b$的值，这样可以使得$f(x) = w^Tx + b$的值任意大，亦即函数间隔$\\hat{\\gamma}$可以在超平面保持不变的情况下被取得任意大。但几何间隔因为除上了$||w||$，使得在缩放$w$和$b$的时候几何间隔$\\tilde{\\gamma}$的值是不会改变的，它只随着超平面的变动而变动，因此，这是更加合适的一个间隔。换言之，这里要找的最大间隔分类超平面中的“间隔”指的是几何间隔。\n",
    "\n",
    "于是最大间隔分类器（maximum margin classifier）的目标函数可以定义为：\n",
    "\n",
    "$$\n",
    "\\max\\tilde{\\gamma}\n",
    "$$\n",
    "\n",
    "同时需满足一些条件，根据间隔的定义，有:\n",
    "\n",
    "$$\n",
    "y_i(w^Tx_i + b) = \\hat{\\gamma{_i}} \\geqslant \\hat{\\gamma}, i = 1,...,n\n",
    "$$\n",
    "\n",
    "回顾下几何间隔的定义, $\\tilde{\\gamma} = y\\gamma{} = \\frac{\\hat{\\gamma}}{||w||}$\n",
    "如果令函数间隔$\\hat{\\gamma}$等于1(之所以令等于1，是为了方便推导和优化，且这样做对目标函数的优化没有影响),则有\n",
    "$\\tilde{\\gamma} = \\frac{1}{||w||}$且$y_i(w^Tx_i + b) \\geqslant 1, i=1,...,n$从而上述目标函数转换成为了\n",
    "$$\n",
    "\\max \\frac{1}{||w||}, s.t.y_i(w^Tx_i + b) \\geqslant 1, i=1,...,n\n",
    "$$\n",
    "如下图所示，中间的实线便是寻找到的**最优超平面（Optimal Hyper Plane）**，其到两条虚线边界的距离相等，这个距离便是几何间隔$\\tilde{\\gamma}$，两条虚线间隔边界之间的距离等于$2\\tilde{\\gamma}$，而**虚线间隔边界上的点则是支持向量**。由于这些支持向量刚好在虚线间隔边界上，所以它们满足$y(w^Tx + b) = 1$（还记得我们把 functional margin 定为 1 了吗？上节中：出于方便推导和优化的目的，我们可以令$\\hat{\\gamma} = 1$），而对于**所有不是支持向量的点，则显然有$y(w^Tx + b) > 1$。**\n",
    "\n",
    "![Text](svm_max_margin.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 深入SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从线性可分到线性不可分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 从原始问题到对偶问题的求解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之前得到的目标函数：\n",
    "$$\n",
    "\\max \\frac{1}{||w||}, s.t.y_i(w^Tx_i + b) \\geqslant 1, i=1,...,n\n",
    "$$\n",
    "求$\\max \\frac{1}{||w||}$的最大值相当于求$\\frac{1}{2}||w||^2$的最小值，所以上述目标函数等价于（$w$由分母变成分子，从而也有原来的max问题变为min问题，很明显，两者问题等价）\n",
    "$$\n",
    "\\min \\frac{1}{2}||w||^2, s.t.y_i(w^Tx_i + b) \\geqslant 1, i=1,...,n\n",
    "$$\n",
    "因为现在的目标函数是二次的，约束条件是线性的，所以它是一个凸二次规划问题。这个问题可以用现成的QP (Quadratic Programming) 优化包进行求解。一言以蔽之：在一定的约束条件下，目标最优，损失最小。\n",
    "\n",
    "此外，由于这个问题的特殊结构，还可以通过拉格朗日对偶性（Lagrange Duality）变换到对偶变量 (dual variable) 的优化问题，即通过求解与原问题等价的对偶问题（dual problem）得到原始问题的最优解，这就是线性可分条件下支持向量机的对偶算法，这样做的优点在于：一者对偶问题往往更容易求解；二者可以自然的引入核函数，进而推广到非线性分类问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 拉格朗日对偶性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简单来讲，通过给每一个约束条件加上一个拉格朗日乘子（Lagrange multiplier）$\\alpha$，定义拉格朗日函数（通过拉格朗日函数将约束条件融合到目标函数里去，从而只用一个函数表达式便能清楚的表达出我们的问题）：\n",
    "$$\n",
    "    \\mathcal{L}(w, b, \\alpha) = \\frac{1}{2}||w||^2 - \\sum_{i = 1}^{n} \\alpha_i(y_i(w^Tx_i+b) - 1)\n",
    "$$\n",
    "然后令$\\theta(w) = \\max \\limits_{\\alpha_i \\geqslant 0} \\mathcal{L}(w, b, \\alpha)$ 容易验证，当某个约束条件不满足时，例如$y_i(w^Tx_i+b) < 1$那么显然有$\\theta(w) = \\infty$。而当所有约束条件都满足时，则最优值为$\\theta(w) = \\frac{1}{2}||w||^2$，亦即最初要最小化的量。\n",
    "\n",
    "因此，在要求约束条件得到满足的情况下最小化$\\frac{1}{2}||w||^2$，实际上等价于直接最小化$\\theta(w)$（当然，这里也有约束条件，就是$\\alpha_{i} \\geqslant 0,i=1,…,n$），因为如果约束条件没有得到满足，$\\theta(w)$会等于无穷大，自然不会是我们所要求的最小值。\n",
    "\n",
    "具体写出来，目标函数变成了：\n",
    "$$\n",
    "\\min \\limits_{(w,b)}\\theta(w)=\\min\\limits_{(w,b)}\\max\\limits_{\\alpha_i \\geqslant 0}\\mathcal{L}(w,b, \\alpha)=p^*\n",
    "$$\n",
    "\n",
    "这里$p^*$用表示这个问题的最优值，且和最初的问题是等价的。如果直接求解，那么一上来便得面对$w$和$b$两个参数，而$\\alpha_i$又是不等式约束，这个求解过程不好做。不妨把最小和最大的位置交换一下，变成：\n",
    "$$\n",
    "\\max\\limits_{\\alpha_i \\geqslant 0}\\min\\limits_{(w,b)}\\mathcal{L}(w,b, \\alpha) = d^*\n",
    "$$\n",
    "\n",
    "交换以后的新问题是原始问题的对偶问题，这个新问题的最优值用$d^*$来表示。而且有$d^* \\leqslant p^*$，在满足某些条件的情况下，这两者相等，这个时候就可以通过求解对偶问题来间接地求解原始问题。\n",
    "\n",
    "换言之，之所以从minmax的原始问题$p^*$，转化为maxmin的对偶问题$d^*$，一者因为$d^*$是$p^*$的近似解，二者，转化为对偶问题后，更容易求解。下面可以先求$L$对$w$、$b$的极小，再求$L$对$\\alpha$的极大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KKT条件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上文中提到“$d^* \\leqslant p^*$在满足某些条件的情况下，两者等价”，这所谓的“满足某些条件”就是要满足KKT条件。\n",
    "\n",
    "一般地，一个最优化数学模型能够表示成下列标准形式：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min f(x)\\\\\n",
    "&h_j(x) = 0, j = 1,...,p\\\\\n",
    "&g_k(x) \\leqslant 0, k = 1,...,q\\\\\n",
    "&x \\in X \\subset R^n\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "其中，$f(x)$是需要最小化的函数，$h(x)$是等式约束，$g(x)$是不等式约束，$p$和$q$分别为等式约束和不等式约束的数量。同时，需要明白以下两点：\n",
    "\n",
    "* 凸优化：$\\mathcal{X}\t\\subset R^n$为一个凸集，$\\mathit{f}:\\mathcal{X} \\rightarrow \\subset R$为一凸函数。凸优化就是要找出一点 $x^\\ast \\in \\mathcal{X}$ ，使得每一 $x \\in \\mathcal{X}$ 满足 $f(x^\\ast)\\le f(x)$ 。\n",
    "* KKT条件的意义：它是非线性规划（Nonlinear Programming）问题能有最优解的必要和充分条件。\n",
    "\n",
    "而KKT条件就是指上面最优化数学模型的标准形式中的最小点$x^*$必须满足下面的条件：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&h_j(x_*) = 0, j = 1,...,p\\\\\n",
    "&g_k(x_*) \\leqslant 0, k = 1,...,q\\\\\n",
    "&\\nabla{f(x_*)} + \\sum_{j=1}^{p} \\lambda_{j} \\nabla{h_j(x_*)} + \\sum_{k=1}^{q} \\mu_{k} \\nabla{g_k(x_*)} = 0\\\\\n",
    "&\\lambda_{j} \\neq 0, \\mu_{k} \\geq 0, \\mu_{k}g_k(x_*) = 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "经过论证，我们这里的问题是满足 KKT 条件，因此现在我们便转化为求解第二个问题。\n",
    "也就是说，原始问题通过满足KKT条件，已经转化成了对偶问题。而求解这个对偶学习问题，分为3个步骤：首先要让$\\mathcal{L}(w,b, \\alpha)$ 关于$w$和$b$最小化，然后求对$\\alpha$的极大，最后利用SMO算法求解对偶问题中的拉格朗日乘子。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 对偶问题求解的3个步骤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 首先固定$\\alpha$，要让$L$关于$w$和$b$最小化，我们分别对$w$，$b$求偏导数，即令$\\partial{\\mathcal{L}}/\\partial{\\mathcal{w}}$和$\\partial{\\mathcal{L}}/\\partial{\\mathcal{b}}$等于零。\n",
    "\n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "    &\\frac{\\partial{\\mathcal{L}}}{\\partial{\\mathcal{w}}} = 0 \\Rightarrow \\mathcal{w} = \\sum_{i=1}^{n} \\alpha_{i}y_{i}x_{i}\\\\\n",
    "    &\\frac{\\partial{\\mathcal{L}}}{\\partial{\\mathcal{b}}} = 0 \\Rightarrow \\mathcal{w} = \\sum_{i=1}^{n} \\alpha_{i}y_{i} = 0\n",
    "    \\end{aligned}\n",
    "    $$\n",
    "\n",
    "    将以上结果代入之前的$\\mathcal{L}$：\n",
    "\n",
    "    $$\\mathcal{L}(w,b, \\alpha) = \\frac{1}{2}||w||^2 - \\sum_{i = 1}^{n} \\alpha_i(y_i(w^Tx_i+b) - 1)$$\n",
    "\n",
    "    得到\n",
    "\n",
    "    $$\\mathcal{L}(w,b, \\alpha) = \\sum_{i = 1}^{n} \\alpha_{i} - \\frac{1}{2}\\sum_{i,j = 1}^{n}\\alpha_{i}\\alpha_{j}y_{i}y_{j}x_{i}^{T}x_{j}$$\n",
    "\n",
    "    此时的拉格朗日函数只包含了一个变量$\\alpha_i$，那就是$\\alpha_i$（求出了便能求出$w$和$b$，由此可见核心问题：分类函数$f(x)=w^Tx + b$也就可以轻而易举的求出来了）。\n",
    "\n",
    "2. 求对$\\alpha$的极大，即是关于对偶问题的最优化问题。经过上面第一个步骤的求$w$和$b$，得到的拉格朗日函数式子已经没有了变量$w$，$b$，只有$\\alpha$。从上面的式子得到：\n",
    "\n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "    \\max_{\\alpha}\\sum_{i=1}^{n}\\alpha_{i} &- \\frac{1}{2}\\sum_{i,j=1}^n\\alpha_i\\alpha_jy_iy_jx_i^Tx_j\\\\\n",
    "    s.t., &\\alpha_{i} \\geqslant 0, i = 1,...,n\\\\\n",
    "    &\\sum_{i=1}^{n}\\alpha_{i}*y_{i} = 0\n",
    "    \\end{aligned}\n",
    "    $$\n",
    "    \n",
    "3. 在求得$\\mathcal{L}(w,b, \\alpha)$关于$w$和$b$最小化，以及对$\\alpha$的极大之后，最后一步则可以利用SMO算法求解对偶问题中的拉格朗日乘子$\\alpha$。\n",
    "上述式子要解决的是在参数${\\alpha_1,\\alpha_2,...,\\alpha_n}$上求最大值$W$的问题，至于$x^{(i)}$和$y^{(i)}$都是已知数。\n",
    "\n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "    \\max_{\\alpha}\\sum_{i=1}^{n}\\alpha_{i} &- \\frac{1}{2}\\sum_{i,j=1}^n\\alpha_i\\alpha_jy_iy_j<x_i \\cdot x_j>\\\\\n",
    "    s.t., &\\alpha_{i} \\geqslant 0, i = 1,...,n\\\\\n",
    "    &\\sum_{i=1}^{n}\\alpha_{i}y_{i} = 0\n",
    "    \\end{aligned}\n",
    "    $$\n",
    "    \n",
    "    等价于求解\n",
    "    \n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "    \\min_{\\alpha} &\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^n\\alpha_i\\alpha_jy_iy_j<x_i \\cdot x_j> - \\sum_{i=1}^{n}\\alpha_{i}\\\\\n",
    "    s.t., &\\alpha_{i} \\geqslant 0, i = 1,...,n\\\\\n",
    "    &\\sum_{i=1}^{n}\\alpha_{i}y_{i} = 0\n",
    "    \\end{aligned}\n",
    "    $$    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMO算法的推导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，来定义特征到结果的输出函数：$u = \\vec{w} \\cdot \\vec{x} - b$ ($u$与我们之前定义的$f(x) = w^Tx + b$实质是一样的)\n",
    "\n",
    "接着，重新定义下咱们原始的优化问题，权当重新回顾，如下：\n",
    "$$\n",
    "\\min \\frac{1}{2}||w||^2, s.t.y_i(\\vec{w_i} \\cdot \\vec{x_i} - b), \\forall{i}\n",
    "$$\n",
    "使用拉格朗日乘子转换为对偶问题后，得到：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_{\\alpha}\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^n&\\alpha_i\\alpha_jy_iy_j<x_i \\cdot x_j>  - \\sum_{i=1}^{n}\\alpha_{i}\\\\\n",
    "&\\alpha_{i} \\geqslant 0, \\forall{i}\\\\\n",
    "&\\sum_{i=1}^{n}\\alpha_{i}y_{i} = 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "经过加入松弛变量后，模型修改为：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_{w,b,\\xi} & \\frac{1}{2}||w||^2 + C\\sum_{i = 1}^{n} \\xi_i\\\\\n",
    "               & y_i(\\vec{w_i} \\cdot \\vec{x_i} - b) \\geqslant 1 - \\xi_i, \\forall{i}\\\\\n",
    "               & 0 \\leqslant \\alpha_{i} \\leqslant C_{i}, \\forall{i}\n",
    "\\end{aligned}\n",
    "$$\n",
    "从而最终我们的问题变为：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_{\\alpha}\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^n&\\alpha_i\\alpha_jK(\\vec{x_i}, \\vec{x_j})y_iy_j  - \\sum_{i=1}^{n}\\alpha_{i}\\\\\n",
    "&0 \\leqslant \\alpha_{i} \\leqslant C_{i}, \\forall{i}\\\\\n",
    "&\\sum_{i=1}^{n}\\alpha_{i}y_{i} = 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "下面要解决的问题是：在$\\alpha = {\\alpha_1, \\alpha_2,...,\\alpha_n}$上求上述目标函数的最小值。为了求解这些乘子，每次从中任意抽取两个乘子$\\alpha_{1}$和$\\alpha_{2}$，然后固定$\\alpha_{1}$和$\\alpha_{2}$以外的其它乘子，使得目标函数只是关于$\\alpha_{1}$和$\\alpha_{2}$的函数。这样，不断的从一堆乘子中任意抽取两个求解，不断的迭代求解子问题，最终达到求解原问题的目的。\n",
    "\n",
    "原对偶问题的子问题的目标函数可以表达为：\n",
    "\n",
    "$$\n",
    "\\Psi = \\frac{1}{2}K_{11}\\alpha_1^2 + \\frac{1}{2}K_{22}\\alpha_2^2 + sK_{12}\\alpha_1\\alpha_2+y_1\\alpha_1\\nu_1+y_2\\alpha_2\\nu_2 - \\alpha_1 - \\alpha_2 + \\Psi_{constant}\n",
    "$$\n",
    "\n",
    "其中\n",
    "$$\n",
    "K_{i,j} = K(\\vec{x_i}, \\vec{x_j})\\\\\n",
    "\\nu_i = \\sum_{j=3}^n y_i\\alpha_j^*K_{ij} = u_i + b^* - y_1\\alpha_1^*K_{1i} - y_2\\alpha_2^*K_{2i}\n",
    "$$\n",
    "\n",
    "为了解决这个子问题，首要问题便是每次如何选取$\\alpha_1$和$\\alpha_2$。实际上，其中一个乘子是违法KKT条件最严重的，另外一个乘子则由另一个约束条件选取。根据KKT条件可以得出目标函数中$\\alpha_i$取值的意义：\n",
    "$$\n",
    "    \\alpha_i = 0 \\Longleftrightarrow y_iu_i \\geqslant 1 \\\\\n",
    "    0 < \\alpha_{i} < C \\Longleftrightarrow y_iu_i = 1\\\\\n",
    "    \\alpha_{i} = C \\Longleftrightarrow y_iu_i \\leqslant 1\n",
    "$$\n",
    "\n",
    "这里的$\\alpha_i$还是拉格朗日乘子：\n",
    "\n",
    "对于第1种情况，表明是正常分类，在间隔边界内部（我们知道正确分类的点）\n",
    "\n",
    "对于第2种情况，表明了是支持向量，在间隔边界上；\n",
    "\n",
    "对于第3种情况，表明了是在两条间隔边界之间；\n",
    "\n",
    "而最优解需要满足KKT条件，即上述3个条件都得满足，以下几种情况出现将会出现不满足：\n",
    "\n",
    "1. $y_iu_i\\leqslant1$但是$\\alpha_i <C$则是不满足的，而原本$\\alpha_i=C$\n",
    "2. $y_iu_i\\geqslant1$但是$\\alpha_i >0$则是不满足的，而原本$\\alpha_i=0$\n",
    "3. $y_iu_i=1$但是$\\alpha_i=0$或者$\\alpha_i=C$则是不满足的，而原本$0<\\alpha_i<C$\n",
    "\n",
    "也就是说，如果存在不满足KKT条件的$\\alpha_i$，那么需要更新这些$\\alpha_i$，这是第一个约束条件。此外，更新的同时还要受到第二个约束条件的限制，即:\n",
    "\n",
    "$$\\sum_{i=1}^ny_i\\alpha_j=0$$\n",
    "\n",
    "因此，如果假设选择的两个乘子$\\alpha_1$和$\\alpha_2$，它们在更新之前分别是$\\alpha_1^{old}$、$\\alpha_2^{old}$，更新之后分别是$\\alpha_1^{new}$、$\\alpha_2^{new}$，那么更新前后的值需要满足以下等式才能保证和为0的约束：\n",
    "\n",
    "$$\\alpha_1^{new}y_1 + \\alpha_2^{new}y_2 = \\alpha_1^{old}y_1 + \\alpha_2^{old}y_2 = \\zeta$$\n",
    "\n",
    "其中，$\\zeta$是常数。\n",
    "\n",
    "两个因子不好同时求解，所以可先求第二个乘子$\\alpha_2$的解$(\\alpha_2^{new})$，再用的$\\alpha_2$的解$(\\alpha_2^{new})$表示$\\alpha_1$的解$(\\alpha_1^{new})$。\n",
    "\n",
    "为了求解$(\\alpha_2^{new})$，得先确定$(\\alpha_2^{new})$的取值范围。假设它的上下边界分别为$H$和$L$，那么有$L \\leqslant \\alpha_2^{new} \\leqslant H $"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
