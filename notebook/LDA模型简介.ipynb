{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性判别分析Linear Discriminant Analysis (LDA)模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "判别分析（Discriminant Analysis）是一种分类方法，它通过一个已知类别的“训练样本”来建立判别准则，并通过预测变量来为未知类别的数据进行分类。线性判别式分析（Linear Discriminant Analysis，简称为LDA）是其中一种，也是模式识别的经典算法，在1996年由Belhumeur引入模式识别和人工智能领域。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数学原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA以Bayes判别思想为基础，当分类只有两种且总体服从多元正态分布条件下，Bayes判别与Fisher判别、距离判别是等价的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基本思想"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基本思想是将高维的模式样本投影到最佳鉴别矢量空间，以达到抽取分类信息和压缩特征空间维数的效果，投影后保证模式样本在新的子空间有最大的类间距离和最小的类内距离，即模式在该空间中有最佳的可分离性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA与PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 出发思想不同 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA：从特征的协方差角度，去找到比较好的投影方式，即选择样本点投影具有最大方差的方向。\n",
    "LDA：考虑分类标签信息，寻求投影后不同类别之间数据点距离最大和同一类别数据点距离最小的方向，即选择分类性能最好的方向。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学习模式不同"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA：无监督式学习，大多场景下只作为数据处理过程的一部分，需要与其他算法结合使用，例如将PCA与聚类、判别分析、回归分析等。\n",
    "LDA：监督式学习，本身除了可以降维外，还可以进行预测应用，因此既可以组合其他模型一起使用，也可以独立使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 降维后可用维度数量不同"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA：降维后最多可生成C-1维子空间（分类标签数-1），因此LDA与原始维度数量无关，只有数据标签分类数量有关。\n",
    "PCA：最多有n维度可用，即最大可以选择全部可用维度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Text](pca_vs_lda.png)\n",
    "上图左侧是PCA的降维：它所作的只是将整组数据整体映射到最方便表示这组数据的坐标轴上，映射时没有利用任何数据内部的分类信息。因此，虽然PCA后的数据在表示上更加方便（降低了维数并能最大限度的保持原有信息）\n",
    "上图右侧是LDA的降维：LDA充分利用了数据的分类信息，将两组数据映射到了另外一个坐标轴上，使得数据更易区分（在低维上就可以区分，减少了运算量）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA的问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 当样本数量远小于样本的特征维数，样本与样本之间的距离变大使得距离度量失效，使LDA算法中的类内、类间离散度矩阵奇异，不能得到最优的投影方向，在人脸识别领域中表现得尤为突出\n",
    "\n",
    "* LDA不适合对非高斯分布的样本进行降维\n",
    "\n",
    "* LDA在样本分类信息依赖方差而不是均值时，效果不好\n",
    "\n",
    "* LDA可能过度拟合数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA是是一个经典的机器学习算法，它是判别分析中的线性分类器，在很多应用情况下会面临数据稀疏的问题，尤其是在面部识别的场景：数据的维度很可能大于数据的样本量，甚至可能呈几倍的差异。此时，LDA的预测准确率会表现较差，当维度数/样本量达到4倍时，准确率会只有50%左右，解决方法之一是可以对LDA算法进行收缩，Python的SKlearn中的LDA算法支持这一收缩规则。默认情况下，solver的值被设定为“svd”，这在大数据量下的表现很好，但不支持收缩规则；当面临数据稀疏时，我们需要使用“lsqr”或“eigen”，另外，与之配合的是shrinkage参数需要设置成auto以便于算法自动调整收缩值，当然你也可以自己凭借经验将值设定在0~1之间（越大收缩越厉害：0时不收缩，1时意味着对角线方差矩阵将被用作协方差矩阵值的估计）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Text](plot_lda_fault.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
