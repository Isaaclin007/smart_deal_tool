{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性判别分析Linear Discriminant Analysis (LDA)模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "判别分析（Discriminant Analysis）是一种分类方法，它通过一个已知类别的“训练样本”来建立判别准则，并通过预测变量来为未知类别的数据进行分类。线性判别式分析（Linear Discriminant Analysis，简称为LDA）是其中一种，也是模式识别的经典算法，在1996年由Belhumeur引入模式识别和人工智能领域。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基本讲解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二分类问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们觉得原始特征数太多，想将n维特征降到只有一维(LDA映射到的低维空间维度小于等于nlabels−1)，而又要保证类别能够“清晰”地反映在低维数据上，也就是这一维就能决定每个样例的类别。\n",
    "\n",
    "假设用来区分二分类的直线（投影函数)为:$y=w^Tx$注意这里得到的$y$值不是$0/1$值，而是$x$投影到直线上的点到原点的距离。\n",
    "\n",
    "已知数据集, $$ D={x^{(0)},x^{(1)},…,x^{(m)}} $$\n",
    "\n",
    "将$D$按照类别标签划分为两类$D1,D2$, 其中$D1 \\bigcup D2=D,D1 \\bigcap D2 = \\emptyset$\n",
    "\n",
    "定义两个子集的中心：\n",
    "\n",
    "$$ μ_1 = \\frac{1}{n_1} \\sum_{x^{(i)}\\in D1} x^{(i)} $$\n",
    "\n",
    "$$ μ_2 = \\frac{1}{n_2} \\sum_{x^{(i)}\\in D2} x^{(i)} $$\n",
    "\n",
    "则两个子集投影后的中心为 \n",
    "\n",
    "$$ \\widetilde{μ_1} = \\frac{1}{n_1} \\sum_{y^{(i)}\\in \\widetilde{D1}} w^T x^{(i)} $$\n",
    "\n",
    "$$ \\widetilde{μ_2} = \\frac{1}{n_2} \\sum_{y^{(i)}\\in \\widetilde{D2}} w^T x^{(i)} $$\n",
    "\n",
    "则两个子集投影后的方差分别为\n",
    "\n",
    "$$ \\widetilde{\\sigma_1}^2 = \\frac{1}{n_1} \\sum_{y^{(i)}\\in \\widetilde{D1}} (y^{(i)}− \\widetilde{μ_1})^2\n",
    "                          = \\frac{1}{n_1} \\sum_{x^{(i)}\\in \\widetilde{D1}} (w^{T}x^{i} − w^{T}μ_{1})^2\n",
    "                      = \\frac{1}{n_1} \\sum_{x^{(i)}\\in \\widetilde{D1}} w^{T}(x^{(i)} − μ_{1})(x^{(i)} − μ_{1})^{T}w$$\n",
    "                      \n",
    "同理，\n",
    "$$ \\widetilde{\\sigma_2}^2 = \\frac{1}{n_2} \\sum_{x^{(i)}\\in \\widetilde{D2}} w^{T}(x^{(i)} − μ_{2})(x^{(i)} − μ_{2})^{T}w$$\n",
    "\n",
    "令\n",
    "$$ S_1 = \\frac{1}{n_1} \\sum_{x^{(i)}\\in \\widetilde{D1}} (x^{(i)} − μ_{1})(x^{(i)} − μ_{1})^{T} $$\n",
    "$$ S_2 = \\frac{1}{n_2} \\sum_{x^{(i)}\\in \\widetilde{D2}} (x^{(i)} − μ_{2})(x^{(i)} − μ_{2})^{T} $$\n",
    "\n",
    "则有\n",
    "$$ \\widetilde{\\sigma_1}^2 = w^T S_1 w, \\widetilde{\\sigma_2}^2 = w^T S_2 w$$\n",
    "\n",
    "令\n",
    "$$ \\widetilde{S_1} = \\frac{1}{n_1} \\sum_{y^{(i)}\\in \\widetilde{D1}} (y^{(i)} − \\widetilde{μ_{1}})(y^{(i)} − \\widetilde{μ_{1}})^{T} $$\n",
    "$$ \\widetilde{S_2} = \\frac{1}{n_2} \\sum_{y^{(i)}\\in \\widetilde{D2}} (y^{(i)} − \\widetilde{μ_{2}})(y^{(i)} − \\widetilde{μ_{2}})^{T} $$\n",
    "\n",
    "则有\n",
    "$$ \\widetilde{S_1} = w^T S_1 w; \\widetilde{S_2} = w^T S_2 w $$\n",
    "\n",
    "可以定义损失函数：\n",
    "$$J(w) = \\frac{(\\widetilde{μ_{1}}-\\widetilde{μ_{2}})^2}{\\widetilde{S_1}^2 + \\widetilde{S_2}^2}$$\n",
    "\n",
    "\n",
    "LDA的分类目标：使得类别内的点距离越近(集中)，类别间的点越远越好。分母表示数据被映射到低维空间之后每一个类别内的方差之和，方差越大表示在低维空间(映射后的空间)一个类别内的点越分散，欲使类别内的点距离越近(集中)，分母应该越小越好。分子为在映射后的空间两个类别各自的中心点的距离的平方，欲使类别间的点越远，分子越大越好。故我们最大化J(w)，求出的w就是最优的了。\n",
    "\n",
    "因为\n",
    "$$\n",
    "|\\widetilde{μ_1}−\\widetilde{μ_1}|^2 = w^{T}(μ_1−μ_2)(μ_1−μ_2)^{T}w = w^TS_Bw\n",
    "$$\n",
    "其中\n",
    "$$\n",
    "S_B = (μ_1−μ_2)(μ_1−μ_2)^{T}\n",
    "$$\n",
    "设$S_W=S_1+S_2$\n",
    "\n",
    "则有\n",
    "\n",
    "$$J(w) = \\frac{w^T S_B w}{w^T S_W w}$$\n",
    "\n",
    "使用拉格朗日乘数法，将分母限制为长度为1（用拉格朗日乘子法重要技巧），并作为拉格朗日乘子法的限制条件，带入得到：\n",
    "\n",
    "$$ loss(w) = w^TS_Bw − (λw^TSw − 1) $$\n",
    "\n",
    "将求的最优的$w$为：\n",
    "\n",
    "$$\n",
    "    w = S^{−1}_w(μ_1−μ_2)\n",
    "$$\n",
    "\n",
    "最终，可得$y = w^T x$的最优$w$，对任意的x，可求得y，当y > 0 时，可以分类为一种，< 0 时，分类为另外一种。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多分类问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果时多分类问题，投影到一维已经不能达到好的分离效果。假设我们有$nlabels$个类别，需要$k$维向量(基向量)来做投影。将这$k$维向量表示为:\n",
    "\n",
    "$$W = (w_1,w_2,…,w_k), w_i(i=1,2,…,k)是列向量$$\n",
    "\n",
    "则有\n",
    "\n",
    "$$y_i=w_{i}^T{i}x,  y=W^{T}x$$\n",
    "\n",
    "将$D$按照类别标签划分为$D_{nlabels}$类$D_1,D_2,…,D_{nlabels}$, 其中\n",
    "\n",
    "将$D$按照类别标签划分为两类$D1,D2,…,D_{nlabels}$, 其中\n",
    "\n",
    "$$\n",
    "D1 \\bigcup D2 \\bigcup ... \\bigcup D_{nlabels} =D,D1 \\bigcap D2 \\bigcap ... \\bigcap D_{nlabels} = \\emptyset\n",
    "$$\n",
    "\n",
    "定义每个子集的中心：\n",
    "\n",
    "$$ μ_i = \\frac{1}{n_i} \\sum_{x^{(i)}\\in Di} x^{(i)} $$\n",
    "\n",
    "总体样本均值：\n",
    "\n",
    "$$ μ = \\frac{1}{m} \\sum_{x^{(i)}\\in Di} x^{(i)} $$\n",
    "\n",
    "类内离散度矩阵定义类似：\n",
    "\n",
    "$$\n",
    "    S_w = \\sum_{i = 1}^{nlabels} \\sum_{x^{k} \\in D_i} (x^{(k)} - μ_i)(x^{(k)} - μ_i)^T\n",
    "$$\n",
    "\n",
    "类间离散度矩阵的定义:\n",
    "原来度量的是两个均值点的散列情况，现在度量的是每类均值点相对于样本中心的散列情况。类似于将$μ_i$看作样本点，$μ$看作均值的协方差矩阵，如果某类里面的样本点较多，那么其权重稍大，权重用$\\frac{n_i}{m}$，但由于$J(w)$对倍数不敏感，因此使用$n_i$，$S_B$的具体定义如下：\n",
    "\n",
    "$$\n",
    "S_B = \\sum_{i = 1}^{nlabels}n_i(μ_i - μ)(μ_i - μ)^T\n",
    "$$\n",
    "\n",
    "矩阵$(μ_i−μ)(μ_i−μ^)T$的实际意义是一个协方差矩阵，这个矩阵所刻画的是该类与样本总体之间的关系，其中该矩阵对角线上的函数所代表的是该类相对样本总体的方差(即分散度)，而非对角线上的元素所代表是该类样本总体均值的协方差（即该类和总体样本的相关联度或称冗余度）\n",
    "\n",
    "LDA做为一个分类的算法，我们当然希望它所分的类之间耦合度低，类内的聚合度高，即类内离散度矩阵的中的数值要小，而类间离散度矩阵中的数值要大，这样的分类的效果才好。\n",
    "\n",
    "$$J(w) = \\frac{\\vert{}w^T S_B w\\vert{}}{\\vert{}w^T S_W w\\vert{}}$$\n",
    "\n",
    "分子分母都是散列矩阵，要将矩阵变成实数，需要取行列式。又因为行列式的值实际上是矩阵特征值的积，一个特征值可以表示在该特征向量上的发散程度。\n",
    "\n",
    "$$ W^* = argmax \\frac{\\vert{}w^T S_B w\\vert{}}{\\vert{}w^T S_W w\\vert{}} $$\n",
    "根据拉格朗日乘数法可知，最佳投影矩阵$W^*$的列向量满足下面特征方程\n",
    "$$ S_BW = λS_wW, $$\n",
    "可得：\n",
    "$$W^*=argmax\\vert{}λ\\vert{}$$\n",
    "\n",
    "只需要取绝对值比较大的$k$个特征值(矩阵S_w^{-1}S_B的特征值)所对应的特征向量，而这$k$个向量构成的低维空间就是我们需要找的低维空间。\n",
    "\n",
    "由于$S^{−1}_{w}S_B$不一定是对称阵，因此，得到的$k$个特征向量不一定正交，这也是与PCA不同的地方。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA算法总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA算法既可以用来降维，又可以用来分类。\n",
    "\n",
    "* 优点：\n",
    "    * LDA是监督学习，在分类和降维过程中利用了类别的先验知识，而PCA这样的无监督学习则无法使用类别先验知识\n",
    "    * LDA在样本分类信息依赖均值而不是方差的时候，比PCA之类的降维算法效果更好\n",
    "\n",
    "* 缺点：\n",
    "    * LDA至多投影到nlabels−1维子空间\n",
    "    * LDA不适合对非高斯分布样本进行降维 ![Text](lda_ungussian.png)上图中红色区域表示一类样本，蓝色区域表示另一类，由于是2类，所以最多投影到1维上。不管在直线上怎么投影，都难使红色点和蓝色点内部凝聚，类间分离。\n",
    "    * LDA在样本分类信息依赖方差而不是均值时，效果不好 ![Text](lda_unvariance.png)上图中，样本点依靠方差信息进行分类，而不是均值信息。LDA不能够进行有效分类，因为LDA过度依靠均值信息。\n",
    "    * LDA在非线性情形效果不好"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数学原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA以Bayes判别思想为基础，当分类只有两种且总体服从多元正态分布条件下，Bayes判别与Fisher判别、距离判别是等价的。\n",
    "\n",
    "基本思想是将高维的模式样本投影到最佳鉴别矢量空间，以达到抽取分类信息和压缩特征空间维数的效果，投影后保证模式样本在新的子空间有最大的类间距离和最小的类内距离，即模式在该空间中有最佳的可分离性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性分类判别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于二分类问题，LDA针对的是：数据服从高斯分布，且均值不同，方差相同。概率密度："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Text](lda_probability_function.png)\n",
    "p是数据的维度。\n",
    "\n",
    "分类判别函数：\n",
    "![Text](lda_discriminant_function.png)\n",
    "可以看出结果是关于x的一次函数：$wx+w_0$，线性分类判别的说法由此得来。\n",
    "\n",
    "参数计算：\n",
    "![Text](lda_parameter_compute.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA与PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 出发思想不同 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA：从特征的协方差角度，去找到比较好的投影方式，即选择样本点投影具有最大方差的方向。\n",
    "LDA：考虑分类标签信息，寻求投影后不同类别之间数据点距离最大和同一类别数据点距离最小的方向，即选择分类性能最好的方向。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学习模式不同"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA：无监督式学习，大多场景下只作为数据处理过程的一部分，需要与其他算法结合使用，例如将PCA与聚类、判别分析、回归分析等。\n",
    "LDA：监督式学习，本身除了可以降维外，还可以进行预测应用，因此既可以组合其他模型一起使用，也可以独立使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 降维后可用维度数量不同"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA：降维后最多可生成C-1维子空间（分类标签数-1），因此LDA与原始维度数量无关，只有数据标签分类数量有关。\n",
    "PCA：最多有n维度可用，即最大可以选择全部可用维度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Text](pca_vs_lda.png)\n",
    "上图左侧是PCA的降维：它所作的只是将整组数据整体映射到最方便表示这组数据的坐标轴上，映射时没有利用任何数据内部的分类信息。因此，虽然PCA后的数据在表示上更加方便（降低了维数并能最大限度的保持原有信息）\n",
    "上图右侧是LDA的降维：LDA充分利用了数据的分类信息，将两组数据映射到了另外一个坐标轴上，使得数据更易区分（在低维上就可以区分，减少了运算量）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA的问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 当样本数量远小于样本的特征维数，样本与样本之间的距离变大使得距离度量失效，使LDA算法中的类内、类间离散度矩阵奇异，不能得到最优的投影方向，在人脸识别领域中表现得尤为突出\n",
    "\n",
    "* LDA不适合对非高斯分布的样本进行降维\n",
    "\n",
    "* LDA在样本分类信息依赖方差而不是均值时，效果不好\n",
    "\n",
    "* LDA可能过度拟合数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA是是一个经典的机器学习算法，它是判别分析中的线性分类器，在很多应用情况下会面临数据稀疏的问题，尤其是在面部识别的场景：数据的维度很可能大于数据的样本量，甚至可能呈几倍的差异。此时，LDA的预测准确率会表现较差，当维度数/样本量达到4倍时，准确率会只有50%左右，解决方法之一是可以对LDA算法进行收缩，Python的SKlearn中的LDA算法支持这一收缩规则。默认情况下，solver的值被设定为“svd”，这在大数据量下的表现很好，但不支持收缩规则；当面临数据稀疏时，我们需要使用“lsqr”或“eigen”，另外，与之配合的是shrinkage参数需要设置成auto以便于算法自动调整收缩值，当然你也可以自己凭借经验将值设定在0~1之间（越大收缩越厉害：0时不收缩，1时意味着对角线方差矩阵将被用作协方差矩阵值的估计）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Text](plot_lda_fault.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "#import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "def shuffle_data(X, y, seed=None):\n",
    "    if seed:np.random.seed(seed)\n",
    "    idx = np.arange(X.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    return X[idx], y[idx]\n",
    "\n",
    "# 正规化数据集 X\n",
    "def normalize(X, axis=-1, p=2):\n",
    "    lp_norm = np.atleast_1d(np.linalg.norm(X, p, axis))\n",
    "    lp_norm[lp_norm == 0] = 1\n",
    "    return X / np.expand_dims(lp_norm, axis)\n",
    "\n",
    "# 标准化数据集 X\n",
    "def standardize(X):\n",
    "    X_std = np.zeros(X.shape)\n",
    "    mean = X.mean(axis=0)\n",
    "    std = X.std(axis=0)\n",
    "\n",
    "    # 做除法运算时请永远记住分母不能等于0的情形\n",
    "    # X_std = (X - X.mean(axis=0)) / X.std(axis=0) \n",
    "    for col in range(np.shape(X)[1]):\n",
    "        if std[col]:\n",
    "            X_std[:, col] = (X_std[:, col] - mean[col]) / std[col]\n",
    "\n",
    "    return X_std\n",
    "\n",
    "# 划分数据集为训练集和测试集\n",
    "def train_test_split(X, y, test_size=0.2, shuffle=True, seed=None):\n",
    "    if shuffle:\n",
    "        X, y = shuffle_data(X, y, seed)\n",
    "\n",
    "    n_train_samples = int(X.shape[0] * (1-test_size))\n",
    "    x_train, x_test = X[:n_train_samples], X[n_train_samples:]\n",
    "    y_train, y_test = y[:n_train_samples], y[n_train_samples:]\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "def accuracy(y, y_pred):\n",
    "    y = y.reshape(y.shape[0], -1)\n",
    "    y_pred = y_pred.reshape(y_pred.shape[0], -1)\n",
    "    return np.sum(y == y_pred)/len(y)\n",
    "\n",
    "# 计算矩阵X的协方差矩阵\n",
    "def calculate_covariance_matrix(X, Y=np.empty((0,0))):\n",
    "    if not Y.any():\n",
    "        Y = X\n",
    "    n_samples = np.shape(X)[0]\n",
    "    covariance_matrix = (1 / (n_samples-1)) * (X - X.mean(axis=0)).T.dot(Y - Y.mean(axis=0))\n",
    "    return np.array(covariance_matrix, dtype=float)\n",
    "\n",
    "class BiClassLDA():\n",
    "    \"\"\"\n",
    "    线性判别分析分类算法(Linear Discriminant Analysis classifier). 既可以用来分类也可以用来降维.\n",
    "    此处实现二类情形(二类情形分类)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.w = None\n",
    "\n",
    "    def transform(self, X, y):\n",
    "        self.fit(X, y)\n",
    "        # Project data onto vector\n",
    "        X_transform = X.dot(self.w)\n",
    "        return X_transform\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Separate data by class\n",
    "        X = X.reshape(X.shape[0], -1)\n",
    "\n",
    "        X1 = X[y == 0]\n",
    "        X2 = X[y == 1]\n",
    "        y = y.reshape(y.shape[0], -1)\n",
    "\n",
    "        # 计算两个子集的协方差矩阵\n",
    "        S1 = calculate_covariance_matrix(X1)\n",
    "        S2 = calculate_covariance_matrix(X2)\n",
    "        Sw = S1 + S2\n",
    "\n",
    "        # 计算两个子集的均值\n",
    "        mu1 = X1.mean(axis=0)\n",
    "        mu2 = X2.mean(axis=0)\n",
    "        mean_diff = np.atleast_1d(mu1 - mu2)\n",
    "        mean_diff = mean_diff.reshape(X.shape[1], -1)\n",
    "\n",
    "        # 计算w. 其中w = Sw^(-1)(mu1 - mu2), 这里我求解的是Sw的伪逆, 因为Sw可能是奇异的\n",
    "        self.w = np.linalg.pinv(Sw).dot(mean_diff)\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = []\n",
    "        for sample in X:\n",
    "            sample = sample.reshape(1, sample.shape[0])\n",
    "            h = sample.dot(self.w)\n",
    "            y = 1 * (h[0][0] < 0)\n",
    "            y_pred.append(y)\n",
    "        return y_pred\n",
    "\n",
    "def main():\n",
    "    # 加载数据\n",
    "    data = datasets.load_iris()\n",
    "    X = data.data\n",
    "    y = data.target\n",
    "\n",
    "    # 只取label=0和1的数据，因为是二分类问题\n",
    "    X = X[y != 2]\n",
    "    y = y[y != 2]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "    # 训练模型\n",
    "    lda = BiClassLDA()\n",
    "    lda.fit(X_train, y_train)\n",
    "    lda.transform(X_train, y_train)\n",
    "\n",
    "    # 在测试集上预测\n",
    "    y_pred = lda.predict(X_test)\n",
    "    y_pred = np.array(y_pred)\n",
    "    accu = accuracy(y_test, y_pred)\n",
    "    print (\"Accuracy:\", accu)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
